{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous part, we applied undersampling of the majority class (non-fraud cases) to achieve class balanceness. We trained and tested logistic regression on balanced training and test set, and we were happy with the results.\n",
    "\n",
    "In this part, we'll tackle the problem in a more realistic scenario: the test is unbalanced where we have way more non-fraud cases than the fraud cases. So we need to be careful about how we construct the training set and how the model is trained.\n",
    "\n",
    "Here is what we'll do:\n",
    "1. Construct balanced training set as in the previous part, and apply the model trained on it on the unbalanced test set.\n",
    "2. Construct unbalanced training set with the same level of unbalanceness as test set, and apply the model trained on it on the unbalanced test set.\n",
    "3. Combine (average) the model trained in **1** and **2**, and apply the averaged model on the test set.\n",
    "4. Other than logistic regression, is there any other more sophisticated algorithm (boosting) that we can use ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.classification import GBTClassifier\n",
    "\n",
    "from pyspark.ml.feature import StandardScaler\n",
    "from pyspark.ml.feature import Normalizer\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "\n",
    "from pyspark.ml.linalg import DenseVector\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.types import IntegerType, DoubleType\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "df = spark.read.csv(\"creditcard.csv\", header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As in previous parts, cast the columns into appropriate types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "colnames = [col.name for col in df.schema.fields]\n",
    "for col in colnames[:-1]:\n",
    "    df = df.withColumn(col, df[col].cast(\"float\"))\n",
    "\n",
    "df = df.withColumn(\"Class\", df[\"Class\"].cast(\"int\"))        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define functions for evaluating binary classification performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def binary_classification_eval(df, predictionCol=\"prediction\", labelCol=\"label\"):\n",
    "    acc = df.select([predictionCol, labelCol]).rdd.map(lambda x: 1. if x[0] == x[1] else 0.).mean()\n",
    "    \n",
    "    precision = df.select([predictionCol, labelCol]).rdd.map(lambda x: 1. if (x[0] == 1) and (x[1] == 1) else 0.).sum()/\\\n",
    "    df.select([predictionCol, labelCol]).rdd.map(lambda x: 1. if x[0] == 1 else 0.).sum()\n",
    "    \n",
    "    recall = df.select([predictionCol, labelCol]).rdd.map(lambda x: 1. if (x[0] == 1) and (x[1] == 1) else 0.).sum()/\\\n",
    "    df.select([predictionCol, labelCol]).rdd.map(lambda x: 1. if x[1] == 1 else 0.).sum()\n",
    "    \n",
    "    return {\"accuracy\":acc, \"precision\": precision, \"recall\": recall}\n",
    "\n",
    "def confusion_matrix(df, predictionCol=\"prediction\", labelCol=\"label\"):\n",
    "    cm = np.zeros((2, 2))\n",
    "    cm[0, 0] = df.select([predictionCol, labelCol]).rdd.map(lambda x: 1 if x[0] == 1 and x[1] == 1 else 0).sum()\n",
    "    cm[0, 1] = df.select([predictionCol, labelCol]).rdd.map(lambda x: 1 if x[0] == 0 and x[1] == 1 else 0).sum()\n",
    "    cm[1, 1] = df.select([predictionCol, labelCol]).rdd.map(lambda x: 1 if x[0] == 0 and x[1] == 0 else 0).sum()\n",
    "    cm[1, 0] = df.select([predictionCol, labelCol]).rdd.map(lambda x: 1 if x[0] == 1 and x[1] == 0 else 0).sum()\n",
    "    cm = pd.DataFrame(cm)\n",
    "    cm.index = [\"T\", \"F\"]\n",
    "    cm.index.name = \"Target\"\n",
    "    cm.columns = [\"T\", \"F\"]\n",
    "    cm.columns.name = \"Predicted\"\n",
    "    cm = cm.applymap(int)\n",
    "    return cm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Split the entire data into **train** and **test** partitions with the same positive-to-negative ratio.\n",
    "2. Sample a **balanced** subset from train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[Time: float, V1: float, V2: float, V3: float, V4: float, V5: float, V6: float, V7: float, V8: float, V9: float, V10: float, V11: float, V12: float, V13: float, V14: float, V15: float, V16: float, V17: float, V18: float, V19: float, V20: float, V21: float, V22: float, V23: float, V24: float, V25: float, V26: float, V27: float, V28: float, Amount: float, label: int]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train, test = df.randomSplit([0.6, 0.4], seed=201)\n",
    "\n",
    "train = train.withColumnRenamed(\"Class\", \"label\")\n",
    "train.persist()\n",
    "\n",
    "test = test.withColumnRenamed(\"Class\", \"label\")\n",
    "test.persist()\n",
    "\n",
    "pos = train.filter(train[\"label\"] == 1)\n",
    "neg = train.filter(train[\"label\"] == 0)\n",
    "ratio = pos.count() / float(neg.count())\n",
    "neg = neg.sample(False, ratio, seed=123)\n",
    "balanced = pos.union(neg)\n",
    "balanced.persist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show the percentage of minority class in **train**, **test** and **balanced**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training partition (unbalanced)\n",
      "Num of fraud cases: 287, Total: 170683, Percentage: 0.001681\n",
      "\n",
      "Training partition (balanced)\n",
      "Num of fraud cases: 287, Total: 580, Percentage: 0.494828\n",
      "\n",
      "Test partition\n",
      "Num of fraud cases: 205, Total: 114124, Percentage: 0.001796\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print \"Training partition (unbalanced)\\nNum of fraud cases: %d, Total: %d, Percentage: %f\\n\" % (\n",
    "    train.select(\"label\").rdd.map(lambda x: int(x[0])).sum(),\n",
    "    train.count(),\n",
    "    train.select(\"label\").rdd.map(lambda x: x[0]).mean())\n",
    "\n",
    "print \"Training partition (balanced)\\nNum of fraud cases: %d, Total: %d, Percentage: %f\\n\" % (\n",
    "    balanced.select(\"label\").rdd.map(lambda x: int(x[0])).sum(),\n",
    "    balanced.count(),\n",
    "    balanced.select(\"label\").rdd.map(lambda x: x[0]).mean())\n",
    "\n",
    "print \"Test partition\\nNum of fraud cases: %d, Total: %d, Percentage: %f\\n\" % (\n",
    "    test.select(\"label\").rdd.map(lambda x: int(x[0])).sum(),\n",
    "    test.count(),\n",
    "    test.select(\"label\").rdd.map(lambda x: x[0]).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define function that returns a Spark ML **Model** (result of applying **fit** of an **Estimator** on data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def trainUsing(data, suffix):\n",
    "    assembler = VectorAssembler(inputCols=colnames[:-1],\n",
    "                                outputCol=\"features_%s\" % suffix)\n",
    "    scaler = StandardScaler(inputCol=\"features_%s\" % suffix, outputCol=\"scaledFeatures_%s\" % suffix,\n",
    "                            withStd=True, withMean=True)\n",
    "    lr = LogisticRegression(maxIter=10, featuresCol=\"scaledFeatures_%s\" % suffix,\n",
    "                            predictionCol=\"prediction_%s\" % suffix,\n",
    "                            probabilityCol=\"probability_%s\" % suffix,\n",
    "                            rawPredictionCol=\"rawPrediction_%s\" % suffix,\n",
    "                            regParam=0.01, elasticNetParam=0.8)\n",
    "\n",
    "    pipeline = Pipeline(stages=[assembler, scaler, lr])\n",
    "    plModel = pipeline.fit(data)\n",
    "    return plModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train logistic regression on balanced and unbalanced (i.e. **train**) data, and apply on the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[Time: float, V1: float, V2: float, V3: float, V4: float, V5: float, V6: float, V7: float, V8: float, V9: float, V10: float, V11: float, V12: float, V13: float, V14: float, V15: float, V16: float, V17: float, V18: float, V19: float, V20: float, V21: float, V22: float, V23: float, V24: float, V25: float, V26: float, V27: float, V28: float, Amount: float, label: int, features_balanced: vector, scaledFeatures_balanced: vector, rawPrediction_balanced: vector, probability_balanced: vector, prediction_balanced: double, features_unbalanced: vector, scaledFeatures_unbalanced: vector, rawPrediction_unbalanced: vector, probability_unbalanced: vector, prediction_unbalanced: double]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "balancedModel = trainUsing(balanced, \"balanced\")\n",
    "unbalancedModel = trainUsing(train, \"unbalanced\")\n",
    "\n",
    "test = balancedModel.transform(test)\n",
    "test = unbalancedModel.transform(test)\n",
    "test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Performance of applying the **balancedModel** on the unbalanced test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Area under PR: 0.707464048935\n",
      "Area under ROC: 0.982435187689\n",
      "{'recall': 0.8975609756097561, 'precision': 0.12813370473537605, 'accuracy': 0.9888454663348645}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Predicted</th>\n",
       "      <th>T</th>\n",
       "      <th>F</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Target</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>T</th>\n",
       "      <td>184</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F</th>\n",
       "      <td>1252</td>\n",
       "      <td>112667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Predicted     T       F\n",
       "Target                 \n",
       "T           184      21\n",
       "F          1252  112667"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction_balanced\", labelCol=\"label\")\n",
    "print \"Area under PR:\", evaluator.evaluate(test,\n",
    "                                           {evaluator.labelCol: \"label\", evaluator.metricName: \"areaUnderPR\"})\n",
    "print \"Area under ROC:\", evaluator.evaluate(test,\n",
    "                                            {evaluator.labelCol: \"label\", evaluator.metricName: \"areaUnderROC\"})\n",
    "print binary_classification_eval(test, predictionCol=\"prediction_balanced\")\n",
    "confusion_matrix(test, predictionCol=\"prediction_balanced\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Performance of applying the **unbalancedModel** on the unbalanced test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Area under PR: 0.732983126256\n",
      "Area under ROC: 0.950600844117\n",
      "{'recall': 0.0975609756097561, 'precision': 0.9523809523809523, 'accuracy': 0.99837019382426}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Predicted</th>\n",
       "      <th>T</th>\n",
       "      <th>F</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Target</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>T</th>\n",
       "      <td>20</td>\n",
       "      <td>185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F</th>\n",
       "      <td>1</td>\n",
       "      <td>113918</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Predicted   T       F\n",
       "Target               \n",
       "T          20     185\n",
       "F           1  113918"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction_unbalanced\", labelCol=\"label\")\n",
    "print \"Area under PR:\", evaluator.evaluate(test,\n",
    "                                           {evaluator.labelCol: \"label\", evaluator.metricName: \"areaUnderPR\"})\n",
    "print \"Area under ROC:\", evaluator.evaluate(test,\n",
    "                                            {evaluator.labelCol: \"label\", evaluator.metricName: \"areaUnderROC\"})\n",
    "print binary_classification_eval(test, predictionCol=\"prediction_unbalanced\")\n",
    "confusion_matrix(test, predictionCol=\"prediction_unbalanced\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**balancedModel** resulted in better recall than the **unbalancedModel**, while **unbalancedModel** had better precision than **balancedModel**.\n",
    "\n",
    "This is a dataset about detecting fraudulent cases of credit card transactions, so miss-classifying a fraud case as non-fraud (**False Negative**) should be much worse than miss-classifying non-fraud as fraud (**False Positive**). In this sense, the **balancedModel** should be preferred over **unbalancedModel**.\n",
    "\n",
    "With that being said, we still want a model that can achieve better trade-off between precision and recall. Next, we'll simply try **averaging the prediction of balancedModel and unbalancedModel**:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The raw prediction of the two models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------+------------------------+\n",
      "|rawPrediction_balanced|rawPrediction_unbalanced|\n",
      "+----------------------+------------------------+\n",
      "|  [1.81230462492876...|    [6.41437391569042...|\n",
      "|  [2.04060271859517...|    [6.09943569050005...|\n",
      "|  [2.57045867472748...|    [6.51967462963196...|\n",
      "|  [2.35456939292213...|    [6.48793908689729...|\n",
      "|  [1.09375508677486...|    [6.22885124543104...|\n",
      "|  [1.06607125457434...|    [6.21531543670417...|\n",
      "|  [2.46473647821187...|    [6.54057286342395...|\n",
      "|  [1.24788587210638...|    [6.74475650339504...|\n",
      "|  [2.09052369514980...|    [6.39325253222303...|\n",
      "|  [2.03978568965274...|    [6.49363196521270...|\n",
      "|  [2.10786595514564...|    [6.41143763658613...|\n",
      "|  [1.85737776236148...|    [6.45719962582102...|\n",
      "|  [1.97607746667212...|    [6.82237954177852...|\n",
      "|  [2.00668814388170...|    [6.38470369209450...|\n",
      "|  [3.44888347026610...|    [6.32755726452673...|\n",
      "|  [1.58522714368824...|    [6.44621683407795...|\n",
      "|  [3.00999300340899...|    [6.47138789825092...|\n",
      "|  [1.99905447745872...|    [6.46458550159942...|\n",
      "|  [1.94931295565013...|    [6.48010753475575...|\n",
      "|  [1.91122923625641...|    [6.67722637950256...|\n",
      "+----------------------+------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test.select([\"rawPrediction_balanced\", \"rawPrediction_unbalanced\"]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assemble the relevant columns and combine the prediction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[Time: float, V1: float, V2: float, V3: float, V4: float, V5: float, V6: float, V7: float, V8: float, V9: float, V10: float, V11: float, V12: float, V13: float, V14: float, V15: float, V16: float, V17: float, V18: float, V19: float, V20: float, V21: float, V22: float, V23: float, V24: float, V25: float, V26: float, V27: float, V28: float, Amount: float, label: int, features_balanced: vector, scaledFeatures_balanced: vector, rawPrediction_balanced: vector, probability_balanced: vector, prediction_balanced: double, features_unbalanced: vector, scaledFeatures_unbalanced: vector, rawPrediction_unbalanced: vector, probability_unbalanced: vector, prediction_unbalanced: double, rawPredictionAverage: vector]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "assembler = VectorAssembler(inputCols=[\"rawPrediction_balanced\", \"rawPrediction_unbalanced\", \"label\"],\n",
    "                                outputCol=\"rawPredictionAverage\")\n",
    "\n",
    "test = assembler.transform(test)\n",
    "test.persist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define function that computes the average of model predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def aveargeRawPrediction(row):\n",
    "    array = row[\"rawPredictionAverage\"].toArray()[::2]\n",
    "    label = float(array[-1])\n",
    "    array = array[:-1]\n",
    "    rawPrediction = array.mean()\n",
    "    probability = 1. / (1 + np.exp(-rawPrediction))\n",
    "    prediction = 0.0 if probability >= 0.5 else 1.0\n",
    "\n",
    "    rawPrediction = DenseVector([rawPrediction, -rawPrediction])\n",
    "    probability = DenseVector([probability, 1.-probability])\n",
    "    \n",
    "    return Row(rawPredictionAverage=rawPrediction,\n",
    "               probabilityAverage=probability,\n",
    "               predictionAverage=prediction,\n",
    "               label=label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Column 1 and 2: **balancedModel** prediction\n",
    "* Column 3 and 4: **unbalancedModel** prediction\n",
    "* Column 5: class label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------------------------------------------------+\n",
      "|rawPredictionAverage                                                             |\n",
      "+---------------------------------------------------------------------------------+\n",
      "|[1.812304624928764,-1.812304624928764,6.4143739156904225,-6.4143739156904225,0.0]|\n",
      "|[2.040602718595173,-2.040602718595173,6.0994356905000595,-6.0994356905000595,0.0]|\n",
      "|[2.5704586747274862,-2.5704586747274862,6.519674629631963,-6.519674629631963,0.0]|\n",
      "|[2.3545693929221394,-2.3545693929221394,6.48793908689729,-6.48793908689729,0.0]  |\n",
      "|[1.093755086774863,-1.093755086774863,6.228851245431046,-6.228851245431046,0.0]  |\n",
      "|[1.0660712545743416,-1.0660712545743416,6.21531543670417,-6.21531543670417,0.0]  |\n",
      "|[2.464736478211878,-2.464736478211878,6.540572863423953,-6.540572863423953,0.0]  |\n",
      "|[1.2478858721063864,-1.2478858721063864,6.744756503395048,-6.744756503395048,0.0]|\n",
      "|[2.090523695149805,-2.090523695149805,6.393252532223033,-6.393252532223033,0.0]  |\n",
      "|[2.039785689652747,-2.039785689652747,6.493631965212706,-6.493631965212706,0.0]  |\n",
      "|[2.1078659551456496,-2.1078659551456496,6.411437636586133,-6.411437636586133,0.0]|\n",
      "|[1.8573777623614878,-1.8573777623614878,6.45719962582102,-6.45719962582102,0.0]  |\n",
      "|[1.9760774666721228,-1.9760774666721228,6.82237954177852,-6.82237954177852,0.0]  |\n",
      "|[2.0066881438817075,-2.0066881438817075,6.384703692094509,-6.384703692094509,0.0]|\n",
      "|[3.448883470266107,-3.448883470266107,6.32755726452673,-6.32755726452673,0.0]    |\n",
      "|[1.5852271436882437,-1.5852271436882437,6.446216834077957,-6.446216834077957,0.0]|\n",
      "|[3.0099930034089977,-3.0099930034089977,6.471387898250922,-6.471387898250922,0.0]|\n",
      "|[1.9990544774587202,-1.9990544774587202,6.464585501599422,-6.464585501599422,0.0]|\n",
      "|[1.949312955650135,-1.949312955650135,6.480107534755751,-6.480107534755751,0.0]  |\n",
      "|[1.9112292362564105,-1.9112292362564105,6.677226379502564,-6.677226379502564,0.0]|\n",
      "+---------------------------------------------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test.select([\"rawPredictionAverage\"]).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construct the dataframe **pred** that holds all the relevant columns for evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[label: double, predictionAverage: double, probabilityAverage: vector, rawPredictionAverage: vector]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = spark.createDataFrame(test.rdd.map(lambda row: aveargeRawPrediction(row)))\n",
    "pred.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----------------+-----------------------------------------+----------------------------------------+\n",
      "|label|predictionAverage|probabilityAverage                       |rawPredictionAverage                    |\n",
      "+-----+-----------------+-----------------------------------------+----------------------------------------+\n",
      "|0.0  |0.0              |[0.983910044356488,0.01608995564351201]  |[4.113339270309593,-4.113339270309593]  |\n",
      "|0.0  |0.0              |[0.9832096689527863,0.016790331047213725]|[4.0700192045476165,-4.0700192045476165]|\n",
      "|0.0  |0.0              |[0.9894921231470449,0.010507876852955067]|[4.545066652179725,-4.545066652179725]  |\n",
      "|0.0  |0.0              |[0.9881235963207355,0.011876403679264458]|[4.421254239909715,-4.421254239909715]  |\n",
      "|0.0  |0.0              |[0.9749448904524635,0.025055109547536536]|[3.6613031661029547,-3.6613031661029547]|\n",
      "|0.0  |0.0              |[0.974436488623058,0.025563511376942016] |[3.6406933456392556,-3.6406933456392556]|\n",
      "|0.0  |0.0              |[0.9890418662166888,0.01095813378331123] |[4.502654670817916,-4.502654670817916]  |\n",
      "|0.0  |0.0              |[0.9819486969057171,0.018051303094282933]|[3.996321187750717,-3.996321187750717]  |\n",
      "|0.0  |0.0              |[0.9858234503212984,0.014176549678701633]|[4.241888113686419,-4.241888113686419]  |\n",
      "|0.0  |0.0              |[0.9861661836309382,0.01383381636906178] |[4.266708827432726,-4.266708827432726]  |\n",
      "|0.0  |0.0              |[0.9860695774435623,0.013930422556437727]|[4.259651795865891,-4.259651795865891]  |\n",
      "|0.0  |0.0              |[0.9845912142494684,0.015408785750531617]|[4.157288694091254,-4.157288694091254]  |\n",
      "|0.0  |0.0              |[0.9878623179855499,0.01213768201445009] |[4.399228504225321,-4.399228504225321]  |\n",
      "|0.0  |0.0              |[0.985163188106776,0.014836811893223945] |[4.195695917988108,-4.195695917988108]  |\n",
      "|0.0  |0.0              |[0.9925215291685617,0.007478470831438266]|[4.888220367396419,-4.888220367396419]  |\n",
      "|0.0  |0.0              |[0.9822893886858372,0.01771061131416285] |[4.0157219888831,-4.0157219888831]      |\n",
      "|0.0  |0.0              |[0.9913429837248211,0.008657016275178897]|[4.74069045082996,-4.74069045082996]    |\n",
      "|0.0  |0.0              |[0.9856820520646167,0.014317947935383324]|[4.231819989529071,-4.231819989529071]  |\n",
      "|0.0  |0.0              |[0.985438565790249,0.014561434209750979] |[4.214710245202943,-4.214710245202943]  |\n",
      "|0.0  |0.0              |[0.9865366302149561,0.013463369785043922]|[4.294227807879487,-4.294227807879487]  |\n",
      "+-----+-----------------+-----------------------------------------+----------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pred.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the **averaged model**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Area under PR: 0.74502021644\n",
      "Area under ROC: 0.981824998036\n",
      "{'recall': 0.6, 'precision': 0.8424657534246576, 'accuracy': 0.9990799481266001}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Predicted</th>\n",
       "      <th>T</th>\n",
       "      <th>F</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Target</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>T</th>\n",
       "      <td>123</td>\n",
       "      <td>82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F</th>\n",
       "      <td>23</td>\n",
       "      <td>113896</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Predicted    T       F\n",
       "Target                \n",
       "T          123      82\n",
       "F           23  113896"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"rawPredictionAverage\", labelCol=\"label\")\n",
    "print \"Area under PR:\", evaluator.evaluate(pred,\n",
    "                                           {evaluator.labelCol: \"label\", evaluator.metricName: \"areaUnderPR\"})\n",
    "print \"Area under ROC:\", evaluator.evaluate(pred,\n",
    "                                            {evaluator.labelCol: \"label\", evaluator.metricName: \"areaUnderROC\"})\n",
    "print binary_classification_eval(pred, predictionCol=\"predictionAverage\")\n",
    "confusion_matrix(pred, predictionCol=\"predictionAverage\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that the recall has improved a lot over the **unbalancedModel**, at the cost of a slight decrease of precision.\n",
    "\n",
    "Note that we simply did unweighted average of the prediction. We could also try weighting the raw predictions with non-uniform weights (e.g. $\\lambda, 1 - \\lambda$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "assembler = VectorAssembler(inputCols=colnames[:-1],\n",
    "                            outputCol=\"featuresGBT\")\n",
    "scaler = StandardScaler(inputCol=\"featuresGBT\", outputCol=\"scaledFeaturesGBT\",\n",
    "                        withStd=True, withMean=True)\n",
    "gbtc = GBTClassifier(maxIter=10, featuresCol=\"scaledFeaturesGBT\", maxDepth=5, labelCol=\"label\", predictionCol=\"predictionGBT\", seed=123)\n",
    "\n",
    "pipeline = Pipeline(stages=[assembler, scaler, gbtc])\n",
    "\n",
    "    \n",
    "plModel = pipeline.fit(train)\n",
    "test = plModel.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'recall': 0.751219512195122, 'precision': 0.927710843373494, 'accuracy': 0.9994479688759593}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Predicted</th>\n",
       "      <th>T</th>\n",
       "      <th>F</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Target</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>T</th>\n",
       "      <td>154</td>\n",
       "      <td>51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F</th>\n",
       "      <td>12</td>\n",
       "      <td>113907</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Predicted    T       F\n",
       "Target                \n",
       "T          154      51\n",
       "F           12  113907"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print binary_classification_eval(test, predictionCol=\"predictionGBT\")\n",
    "confusion_matrix(test, predictionCol=\"predictionGBT\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "As we can see, Gradient Boosted Trees achieved much better results than any previous model. However, still 51 out of 205 fraud cases were miss-classified, we could have applied grid search to find optimal hyperparameters that may hopefully lead to better recall while still maintaining similar precision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
